---
title: "Simultaneous Detection of Gradual and Abrupt Structural Changes in Bayesian Longitudinal Modelling: A Tutorial"
subtitle: "Growth Curve Model with 2-Regime Switching (GCM-RS2)"
author: "Yanling Li, Xiaoyue Xiong, Zita Oravecz & Sy-Miin Chow"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    code_folding: show
    theme: cosmo
    highlight: tango
  pdf_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE, 
  warning = FALSE,
  cache = TRUE,
  fig.width = 8, 
  fig.height = 6
)
```

# Introduction

This tutorial demonstrates the implementation of the **Growth Curve Model with 2-Regime Switching (GCM-RS2)**, a Bayesian longitudinal model that simultaneously captures gradual changes (via growth curve modeling) and abrupt structural changes (via regime switching). This model is particularly useful for analyzing intensive longitudinal data where both slow developmental trends and sudden state transitions may occur.

## Model Overview

The GCM-RS2 model integrates three key components:

1.  **Growth Curve Model (GCM)**: Captures gradual, period-level changes in baseline levels through person-specific intercepts ($\beta_{0i}$) and slopes ($\beta_{1i}$).

2.  **Autoregressive (AR) Component**: Models temporal dependencies within individuals, where the current observation depends on deviations from the previous time point.

3.  **Regime-Switching (RS) Component**: Allows for abrupt shifts between two latent states (regimes), with transition probabilities governed by covariates.

### Mathematical Formulation

For person $i$ at time $t$ within period $b$, the observation model is:

$$
Y_{i,t} = \gamma_{S_{i,t},i} + \mu_{i,b} + \phi_i (Y_{i,t-1} - \mu_{i,b} - \gamma_{S_{i,t-1},i}) + \epsilon_{i,t}
$$

where:

-   $\mu_{i,b} = \beta_{0i} + \beta_{1i} \cdot \text{time}_b$ is the baseline level for person $i$ in period $b$
-   $\gamma_{S_{i,t},i}$ is the regime-specific shift (with regime indicator $S_{i,t} \in \{1, 2\}$)
-   $\phi_i$ is the person-specific AR coefficient
-   $\epsilon_{i,t} \sim N(0, \sigma^2_{\epsilon,i})$ is the process noise

> **Note on notation**: In the accompanying code (and this tutorial), we use the following correspondences with the paper:
>
> - `intercept[pp,bb]` ($\mu_{i,b} = \beta_{0i} + \beta_{1i} \cdot time_b$) corresponds to the GCM component of $\mu_{i,t}$ in the paper (Equation 3)
> - `gamma[k,pp]` corresponds to $\Delta_{\mu,k} + \zeta_{i,\Delta_\mu}$ in the paper—the regime-specific shift plus person-specific random effect (Equation 5)
> - Specifically, `gamma[1,pp]` = 0 (Regime 1 is the reference level), and `gamma[2,pp]` corresponds to $\Delta_{\mu,2} + \zeta_{i,\Delta_\mu}$ in the paper
> - `gamma0[2]` represents the population mean of the regime shift, $\Delta_{\mu,2}$

The regime transition follows a first-order Markov process with log-odds:

$$
\log \frac{P(S_{i,t}=r|S_{i,t-1}=s)}{P(S_{i,t}=s|S_{i,t-1}=s)} = \alpha_{1,r,s} + \alpha_{2,r,s} \cdot \text{Period2}_t \cdot G3_i
$$

where $G3_i$ is a group indicator and $\text{Period2}_t$ indicates the intervention period.

# Prerequisites

## Required Packages

```{r load-packages}
# Load required packages
library(rjags)      # Interface to JAGS for Bayesian modeling
load.module("dic")  # Load DIC module for model comparison
library(coda)       # MCMC diagnostics and summaries
library(ggplot2)    # Visualization
```

## Post-Processing Functions

The following functions are used for summarizing MCMC output. The `HDIofMCMC` function computes the Highest Density Interval (HDI), and `zcalc` provides comprehensive summary statistics for all monitored parameters.

```{r postcalc-functions}
#' Compute Highest Density Interval (HDI) from MCMC samples
#'
#' @param sampleVec A vector of MCMC samples
#' @param credMass Credible mass for the interval (default: 0.95)
#' @return A vector with HDI lower and upper bounds
HDIofMCMC <- function(sampleVec, credMass = 0.95) {
  sortedPts <- sort(sampleVec)
  ciIdxInc <- ceiling(credMass * length(sortedPts))
  nCIs <- length(sortedPts) - ciIdxInc
  ciWidth <- rep(0, nCIs)
  for (i in 1:nCIs) {
    ciWidth[i] <- sortedPts[i + ciIdxInc] - sortedPts[i]
  }
  HDImin <- sortedPts[which.min(ciWidth)]
  HDImax <- sortedPts[which.min(ciWidth) + ciIdxInc]
  HDIlim <- c(HDImin, HDImax)
  return(HDIlim)
}

#' Comprehensive MCMC Summary Statistics
#'
#' @param samples An mcmc.list object from coda.samples()
#' @param credMass Credible mass for HDI (default: 0.95)
#' @param CI Quantiles for credible intervals (default: c(.025, .975))
#' @param delta Not used in current implementation
#' @param filters Optional character vector to filter parameters by name
#' @return A data.frame with summary statistics for each parameter
zcalc <- function(samples, credMass = 0.95, CI = c(.025, .975), 
                  delta = 0, filters = NULL) {
  sampleCount <- 1
  
  # Ensure samples is a list
  if (!is.mcmc.list(samples)) {
    sampleCount <- length(samples)
  } else {
    samples <- list(samples)
  }
  
  # Ensure filter is a vector
  if (!is.null(filters) && !is.vector(filters)) {
    filters <- c(filters)
  }
  
  # Pre-calculate the number of expected rows
  rowCount <- 0
  drops <- list()
  for (k in 1:sampleCount) {
    drops[[k]] <- c(0)
    if (is.null(filters)) {
      rowCount <- rowCount + nvar(samples[[k]])
    } else {
      nvar_k <- nvar(samples[[k]])
      varnames_k <- varnames(samples[[k]])
      for (l in 1:nvar_k) {
        varname <- varnames_k[[l]]
        isOK <- FALSE
        for (f in 1:length(filters)) {
          isOK <- isOK || regexpr(filters[[f]], varname)[1] > 0
        }
        if (isOK) {
          rowCount <- rowCount + 1
        } else {
          drops[[k]] <- c(drops[[k]], l)
        }
      }
    }
  }
  
  columnNames <- c()
  
  # Pre-allocate the result data frame
  result <- data.frame(
    mean = rep(NaN, rowCount),
    median = rep(NaN, rowCount),
    mode = rep(NaN, rowCount),
    sd = rep(NaN, rowCount),
    hdiLow = rep(NaN, rowCount),
    hdiHigh = rep(NaN, rowCount),
    quantileLow = rep(NaN, rowCount),
    quantileHigh = rep(NaN, rowCount),
    SS = rep(NaN, rowCount),
    ESS = rep(NaN, rowCount),
    RHAT = rep(NaN, rowCount),
    stringsAsFactors = FALSE
  )
  
  currentRow <- 0
  
  # Process each model
  for (k in 1:sampleCount) {
    prefix <- ""
    if (sampleCount > 1) {
      prefix <- paste(k, ".", sep = "")
    }
    
    sample <- samples[[k]]
    variables <- nvar(sample)
    varnames_sample <- varnames(sample)
    iterations <- niter(sample)
    chains <- nchain(sample)
    
    for (j in 1:variables) {
      if (!(j %in% drops[[k]])) {
        currentRow <- currentRow + 1
        
        uvalue <- unlist(sample[, j])
        value <- sample[, j]
        
        columnNames <- c(columnNames, paste(prefix, varnames_sample[[j]], sep = ""))
        
        result[currentRow, "SS"] <- iterations * chains
        result[currentRow, "ESS"] <- as.integer(round(effectiveSize(uvalue), 1))
        result[currentRow, "mean"] <- mean(uvalue)
        result[currentRow, "median"] <- median(uvalue)
        
        mcmcDensity <- density(uvalue)
        result[currentRow, "mode"] <- mcmcDensity$x[which.max(mcmcDensity$y)]
        
        HDI <- HDIofMCMC(uvalue, credMass)
        result[currentRow, "hdiLow"] <- HDI[1]
        result[currentRow, "hdiHigh"] <- HDI[2]
        
        resultCI <- quantile(uvalue, CI)
        result[currentRow, "quantileLow"] <- resultCI[1]
        result[currentRow, "quantileHigh"] <- resultCI[2]
        
        result[currentRow, "sd"] <- sd(uvalue)
        
        # Gelman-Rubin RHAT calculation
        chainmeans <- c()
        chainvars <- c()
        for (i in 1:chains) {
          chain_sum <- sum(value[[i]])
          chain_var <- var(value[[i]])
          chain_mean <- chain_sum / iterations
          chainmeans <- c(chainmeans, chain_mean)
          chainvars <- c(chainvars, chain_var)
        }
        globalmean <- sum(chainmeans) / chains
        globalvar <- sum(chainvars) / chains
        
        # Between-chain variance
        b <- sum((chainmeans - globalmean)^2) * iterations / (chains - 1)
        
        # Marginal posterior variance
        varplus <- (iterations - 1) * globalvar / iterations + b / iterations
        
        # Gelman-Rubin statistic
        rhat <- sqrt(varplus / globalvar)
        result[currentRow, "RHAT"] <- rhat
      }
    }
  }
  
  # Round results
  result <- data.frame(apply(result, 2, function(x) round(x, 4)))
  
  # Rename columns
  if (length(result) > 0) {
    names(result)[names(result) == 'hdiLow'] <- paste(sprintf("%.0f", 
      round(credMass * 100, digits = 2)), "HDI_L", sep = "% ")
    names(result)[names(result) == 'hdiHigh'] <- paste(sprintf("%.0f", 
      round(credMass * 100, digits = 2)), "HDI_H", sep = "% ")
    names(result)[names(result) == 'quantileLow'] <- paste("CrI", 
      sprintf("%.1f%%", round(CI[1] * 100, digits = 3)), sep = " ")
    names(result)[names(result) == 'quantileHigh'] <- paste("CrI", 
      sprintf("%.1f%%", round(CI[2] * 100, digits = 3)), sep = " ")
  }
  
  row.names(result) <- columnNames
  return(result)
}
```

# Part 1: Data Generation

This section describes the data generation process for the GCM-RS2 model. The simulation generates multivariate longitudinal data with both gradual growth trends and regime-switching dynamics.

## Data Generation Function

```{r data-generation-function}
#' Generate Simulated Data for GCM-RS2 Model
#'
#' @param N Number of persons (default: 50)
#' @param O Number of time points per person (default: 100)
#' @param shift Magnitude of regime shift (default: 2)
#' @param seed Random seed for reproducibility
#' @return A list containing simulated data and true parameter values
generate_gcm_rs2_data <- function(N = 50, O = 100, shift = 2, seed = 123) {
  
  set.seed(seed)
  
  # Group indicator: two groups (control vs. intervention)
  G3 <- c(rep(0, N / 2), rep(1, N / 2))
  
  # Time structure: 4 periods
  time <- 0:3
  nrPeriod <- length(time)
  
  # Period indicator (Period 2 is the intervention period)
  Period2 <- c(rep(0, O / 4), rep(1, O / 4), rep(0, O / 4), rep(0, O / 4))
  
  # Observation indices for each period
  nrObs <- c(1, seq(O / 4, O, O / 4))
  
  # ---- True Parameter Values ----
  
  # GCM parameters (baseline)
  beta00 <- 0           # Population mean intercept
  sd_beta0 <- 0.2       # SD of random intercepts
  beta10 <- 0.5         # Population mean slope
  sd_beta1 <- 0.2       # SD of random slopes
  
  # Regime-specific shift parameters (gamma in code corresponds to Δ_μ in paper)
  # gamma0[1] = 0 is the reference (Regime 1); gamma0[2] = shift corresponds to Δ_{μ,2}
  gamma0 <- c(0, shift) # Regime 1 fixed at 0; Regime 2 has shift
  sd_gamma <- c(0, 0.2) # SD of random effects (regime 1 fixed)
  
  # AR parameters
  AR0 <- 0.3            # Population mean AR coefficient
  sd_AR <- 0.1          # SD of random AR coefficients
  
  # Process noise parameters
  sd_noise0 <- 0.5      # Population mean of process noise SD
  sd_sd_noise <- 0.1    # SD of process noise SD
  
  # Regime-switching parameters (log-odds)
  alpha <- array(rep(NA, 2 * 2 * 2), dim = c(2, 2, 2))
  # Fix staying probabilities at 0 (on log-odds scale, reference category)
  for (i in 1:2) {
    alpha[i, 1, 1] <- 0
    alpha[i, 2, 2] <- 0
  }
  # Transition parameters
  alpha[1, 2, 1] <- -0.5  # Baseline log-odds: regime 1 -> regime 2
  alpha[1, 1, 2] <- -0.5  # Baseline log-odds: regime 2 -> regime 1
  alpha[2, 2, 1] <- 1     # Effect of G3*Period2 on 1->2 transition
  alpha[2, 1, 2] <- -1    # Effect of G3*Period2 on 2->1 transition
  
  # ---- Initialize Storage Arrays ----
  
  odds <- array(rep(NA, N * O * 2), dim = c(N, O, 2))
  midx <- matrix(NA, N, O)       # Regime indicator
  p_2s <- matrix(NA, N, O)       # Probability of regime 2
  
  Y <- matrix(NA, N, O)          # Observed variable
  gamma <- matrix(NA, 2, N)      # Person-specific regime effects
  beta0 <- rep(NA, N)            # Person-specific intercepts
  beta1 <- rep(NA, N)            # Person-specific slopes
  AR <- rep(NA, N)               # Person-specific AR coefficients
  sd_noise <- rep(NA, N)         # Person-specific noise SDs
  intercept <- matrix(NA, N, nrPeriod)  # Period-specific intercepts
  
  # ---- Simulate Data ----
  
  for (pp in 1:N) {
    # Initial regime probabilities (more likely to start in regime 1)
    odds[pp, 1, 1] <- 0.9
    odds[pp, 1, 2] <- 0.1
    p_2s[pp, 1] <- odds[pp, 1, 2] / (odds[pp, 1, 1] + odds[pp, 1, 2])
    midx[pp, 1] <- sample.int(n = 2, size = 1, prob = odds[pp, 1, ])
    
    # Generate person-specific parameters
    gamma[1, pp] <- rnorm(1, gamma0[1], sd_gamma[1])
    gamma[2, pp] <- rnorm(1, gamma0[2], sd_gamma[2])
    beta0[pp] <- rnorm(1, beta00, sd_beta0)
    beta1[pp] <- rnorm(1, beta10, sd_beta1)
    AR[pp] <- rnorm(1, AR0, sd_AR)
    sd_noise[pp] <- rnorm(1, sd_noise0, sd_sd_noise)
    
    # Initial observation
    Y[pp, 1] <- rnorm(1, 0, sd_noise[pp])
    
    # Loop over periods and time points
    for (bb in 1:nrPeriod) {
      intercept[pp, bb] <- beta0[pp] + beta1[pp] * time[bb]
      
      for (oo in (nrObs[bb] + 1):nrObs[bb + 1]) {
        # Compute transition odds based on covariates
        # Transition from current regime to regime 1
        odds[pp, oo, 1] <- exp(alpha[1, 1, midx[pp, oo - 1]] + 
                                alpha[2, 1, midx[pp, oo - 1]] * Period2[oo] * G3[pp])
        # Transition from current regime to regime 2
        odds[pp, oo, 2] <- exp(alpha[1, 2, midx[pp, oo - 1]] + 
                                alpha[2, 2, midx[pp, oo - 1]] * Period2[oo] * G3[pp])
        
        # Sample new regime
        midx[pp, oo] <- sample.int(n = 2, size = 1, prob = odds[pp, oo, ])
        p_2s[pp, oo] <- odds[pp, oo, 2] / (odds[pp, oo, 1] + odds[pp, oo, 2])
        
        # Generate observation with AR dynamics
        mu_y <- gamma[midx[pp, oo], pp] + intercept[pp, bb] + 
                AR[pp] * (Y[pp, oo - 1] - intercept[pp, bb] - gamma[midx[pp, oo - 1], pp])
        Y[pp, oo] <- rnorm(1, mu_y, sd_noise[pp])
      }
    }
  }
  
  # Return all simulated data and true parameters
  return(list(
    # Simulated data
    Y = Y,
    N = N,
    O = O,
    G3 = G3,
    time = time,
    nrPeriod = nrPeriod,
    Period2 = Period2,
    nrObs = nrObs,
    midx = midx,
    
    # True parameters
    beta00 = beta00, sd_beta0 = sd_beta0,
    beta10 = beta10, sd_beta1 = sd_beta1,
    AR0 = AR0, sd_AR = sd_AR,
    sd_noise0 = sd_noise0, sd_sd_noise = sd_sd_noise,
    gamma0 = gamma0, sd_gamma = sd_gamma,
    alpha = alpha,
    
    # Person-level parameters
    AR = AR, sd_noise = sd_noise,
    gamma = gamma, beta0 = beta0, beta1 = beta1,
    intercept = intercept
  ))
}
```

## Generate Sample Data

```{r generate-sample-data}
# Generate sample data with N=50 persons and T=100 time points
simdata <- generate_gcm_rs2_data(N = 50, O = 100, shift = 2, seed = 123)

# Display dimensions
cat("Data dimensions:\n")
cat("  Number of persons (N):", simdata$N, "\n")
cat("  Number of time points (O):", simdata$O, "\n")
cat("  Number of periods:", simdata$nrPeriod, "\n")
```

## Visualize Simulated Data

```{r visualize-data, fig.cap="Sample trajectories from simulated data. Top: Time series for selected individuals. Bottom: Regime indicators (1 = low, 2 = high)."}
# Plot sample trajectories
par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))

# Select a few persons to display
sample_ids <- c(1, 26)  # One from each group

# Plot time series
plot(1:simdata$O, simdata$Y[sample_ids[1], ], type = "l", col = "blue",
     ylim = range(simdata$Y[sample_ids, ]), 
     xlab = "Time", ylab = "Y", main = "Sample Trajectories")
lines(1:simdata$O, simdata$Y[sample_ids[2], ], col = "red")
legend("topright", legend = c("Control (G3=0)", "Intervention (G3=1)"),
       col = c("blue", "red"), lty = 1, cex = 0.8)

# Add period boundaries
abline(v = c(25, 50, 75), lty = 2, col = "gray")

# Plot regime indicators
plot(1:simdata$O, simdata$midx[sample_ids[1], ], type = "s", col = "blue",
     ylim = c(0.9, 2.1), xlab = "Time", ylab = "Regime", 
     main = "Regime Indicators", yaxt = "n")
lines(1:simdata$O, simdata$midx[sample_ids[2], ], type = "s", col = "red")
axis(2, at = c(1, 2), labels = c("1 (Low)", "2 (High)"))
abline(v = c(25, 50, 75), lty = 2, col = "gray")
```

# Part 2: Model Fitting with JAGS

## JAGS Model Specification

The following sections walk through the JAGS model implementation of the GCM-RS2 framework step by step. We break the model into logical components and explain each part in detail.

### Overview of the JAGS Model Structure

The JAGS model consists of several interconnected components:

| Component | Description | Key Parameters |
|----|----|----|
| **Initial conditions** | First observation and initial regime | `odds[pp,1,]`, `midx[pp,1]` |
| **Person-specific parameters** | Random effects for each individual | `beta0[pp]`, `beta1[pp]`, `AR[pp]`, `gamma[,pp]` |
| **Regime transitions** | Markov switching between regimes | `odds[pp,tt,]`, `midx[pp,tt]`, `alpha` |
| **Observation model** | How observations are generated | `mus[pp,tt]`, `Y[pp,tt]` |
| **Priors** | Prior distributions for population parameters | Various hyperparameters |

### Initial Conditions and Person-Specific Parameters

The first code block handles initialization for each person `pp`. At time point 1:

-   **Initial regime probabilities**: We set the probability of starting in Regime 1 to 0.9 and Regime 2 to 0.1
-   **Initial regime assignment**: `midx[pp,1]` is drawn from a categorical distribution based on these probabilities
-   **Person-specific parameters**: Each person has their own intercept ($\beta_{0,i}$), slope ($\beta_{1,i}$), AR coefficient ($\phi_i$), process noise ($\sigma_{\epsilon,i}$), and regime shift ($\gamma_{2,i}$)

``` jags
for (pp in 1:P) {

## odds: transition probability matrices 
## midx: regime indicators

# 1st observation - Initial regime probabilities
odds[pp,1,1] <- 0.9    # P(starting in regime 1) = 0.9
odds[pp,1,2] <- 0.1    # P(starting in regime 2) = 0.1

midx[pp,1] ~ dcat(odds[pp,1,])   # Sample initial regime

Y[pp,1] ~ dnorm(0, 1)   # First observation (standardized)

# Person-specific parameters (random effects)
beta0[pp] ~ dnorm(beta00, tau_beta0)     # Random intercept
beta1[pp] ~ dnorm(beta10, tau_beta1)     # Random slope
AR[pp] ~ dnorm(AR0, tau_AR)T(-1,1)       # AR coefficient (constrained for stationarity)
sd_noise[pp] ~ dnorm(sd_noise0, tau_sd_noise)  # Process noise SD
tau_noise[pp] = pow(sd_noise[pp], -2)    # Convert to precision

# Regime-specific shifts (gamma in code = Δ_μ in paper)
gamma[1,pp] = 0                          # Regime 1: reference level (fixed at 0)
gamma[2,pp] ~ dnorm(gamma0[2], tau_gamma[2])  # Regime 2: shift relative to regime 1
```

**Key points about person-specific parameters:**

-   `beta0[pp]` and `beta1[pp]`: Define the GCM trajectory $\mu_{i,b} = \beta_{0,i} + \beta_{1,i} \cdot \text{time}_b$
-   `AR[pp]`: Truncated at (-1, 1) to ensure stationarity of the AR process
-   `gamma[1,pp] = 0`: Regime 1 is the reference category (fixed at 0)
-   `gamma[2,pp]`: The shift when in Regime 2 (corresponds to $\Delta_{\mu,2}$ in the paper)

### Regime Transition Model

The regime transition model is the heart of the RS component. For each time point after the first, we compute the **log-odds of transitioning** between regimes based on:

1.  The **previous regime** `midx[pp,tt-1]`
2.  The **intervention period** indicator `Period2[tt]`
3.  The **group membership** `G3[pp]`

The transition probability structure can be represented as a 2×2 matrix of log-odds, where rows indicate the **previous regime** ($S_{i,t-1}$) and columns indicate the **next regime** ($S_{i,t}$):

| Previous → Next | To Regime 1 | To Regime 2 |
|-----------------|-------------|-------------|
| **From Regime 1** | 0 (reference) | $\alpha_{1,2,1} + \alpha_{2,2,1} \cdot \text{Period2} \cdot G3$ |
| **From Regime 2** | $\alpha_{1,1,2} + \alpha_{2,1,2} \cdot \text{Period2} \cdot G3$ | 0 (reference) |

Note that $\alpha_{k,s,s} = 0$ (staying in the same regime) serves as the reference category.

``` jags
# from T2 to maxT[pp]
 for (bb in 1:nrPeriod){
    
   intercept[pp,bb] = beta0[pp] + beta1[pp]*time[bb]  # GCM baseline for period bb
    
   for(tt in (nrObs[bb]+1):nrObs[bb+1]){

# Transition log-odds from previous regime to each regime
# from regime midx[pp,tt-1] to regime 1
odds[pp,tt,1] <- exp(alpha[1,1,midx[pp,tt-1]] + alpha[2,1,midx[pp,tt-1]]*Period2[tt]*G3[pp])

# from regime midx[pp,tt-1] to regime 2
odds[pp,tt,2] <- exp(alpha[1,2,midx[pp,tt-1]] + alpha[2,2,midx[pp,tt-1]]*Period2[tt]*G3[pp])

midx[pp,tt] ~ dcat(odds[pp,tt,])  # Sample current regime
```

**Understanding the `alpha` parameters:**

-   `alpha[1,r,s]`: **Baseline log-odds** of transitioning from regime `s` to regime `r` (when Period2×G3 = 0)
-   `alpha[2,r,s]`: **Effect of intervention** (Period2×G3) on the log-odds of transitioning from `s` to `r`

For example: - `alpha[1,2,1] = -0.5`: Baseline log-odds of transitioning from Regime 1 to Regime 2 - `alpha[2,2,1] = 1.0`: The intervention group (G3=1) during the intervention period (Period2=1) has increased log-odds of transitioning from Regime 1 to Regime 2

### Observation Model

The observation model generates the actual data. For each time point, the expected value `mus[pp,tt]` combines:

1.  **Regime-specific shift**: `gamma[midx[pp,tt],pp]` — the current regime's effect
2.  **GCM baseline**: `intercept[pp,bb]` — the gradual change component
3.  **AR dynamics**: The AR term captures how the previous deviation from baseline carries forward

$$
Y_{i,t} = \gamma_{S_{i,t},i} + \mu_{i,b} + \phi_i (Y_{i,t-1} - \mu_{i,b} - \gamma_{S_{i,t-1},i}) + \epsilon_{i,t}
$$

``` jags
# Observation model
mus[pp,tt] <- gamma[midx[pp,tt],pp] +        # Current regime shift
              intercept[pp,bb] +              # GCM baseline
              AR[pp] * (Y[pp,tt-1] -          # AR component:
                        intercept[pp,bb] -    #   previous deviation
                        gamma[midx[pp,tt-1],pp])  #   from baseline

Y[pp,tt] ~ dnorm(mus[pp,tt], tau_noise[pp])  # Observation with person-specific noise


} # close loop over time points

}

} # close loop over persons
```

### Prior Specifications

The priors section specifies our prior beliefs about the population-level parameters. We use weakly informative priors to allow the data to drive inference while maintaining numerical stability.

``` jags
# Priors for regime shift parameters
# gamma0[2] corresponds to Δ_{μ,2} in the paper
gamma0[2] ~ dnorm(0, 0.01)T(0,)  # Truncated at 0: regime 2 must be HIGHER than regime 1
tau_gamma[2] ~ dgamma(0.001, 0.001)
sd_gamma[2] = pow(tau_gamma[2], -1/2)
```

**Why `T(0,)` for `gamma0[2]`?** This truncation constrains the mean shift to be positive, ensuring Regime 2 represents a "high" state (e.g., elevated baseline) relative to Regime 1. This is an **identification constraint** — without it, the regimes could swap labels across MCMC iterations.

``` jags
# Priors for AR parameters
AR0 ~ dnorm(0, 1)T(-1, 1)   # Mean AR, constrained for stationarity
sd_AR ~ dunif(0, 1)          # SD of random AR effects
tau_AR = pow(sd_AR, -2)

# Priors for process noise parameters
sd_noise0 ~ dunif(0, 1)      # Mean process noise SD
sd_sd_noise ~ dunif(0, 1)    # SD of process noise SD
tau_sd_noise = pow(sd_sd_noise, -2)
```

``` jags
# Priors for GCM parameters (intercept and slope)
beta00 ~ dnorm(0, 0.01)       # Mean intercept (weak prior, precision = 0.01)
tau_beta0 ~ dgamma(0.001, 0.001)  # Precision of random intercepts
sd_beta0 = pow(tau_beta0, -1/2)

beta10 ~ dnorm(0, 0.01)       # Mean slope
tau_beta1 ~ dgamma(0.001, 0.001)  # Precision of random slopes
sd_beta1 = pow(tau_beta1, -1/2)
```

``` jags
# Priors for regime-switching log-odds coefficients
for(kk in 1:2){
  alpha[kk,2,1] ~ dnorm(0, 0.1)  # Log-odds: regime 1 -> regime 2
  alpha[kk,1,1] <- 0             # Reference: staying in regime 1 (fixed at 0)
  alpha[kk,1,2] ~ dnorm(0, 0.1)  # Log-odds: regime 2 -> regime 1
  alpha[kk,2,2] <- 0             # Reference: staying in regime 2 (fixed at 0)
}

} # end of model
```

**Summary of identification constraints in the RS component:**

| Parameter       | Value          | Purpose                                 |
|-----------------|----------------|-----------------------------------------|
| `alpha[kk,1,1]` | Fixed at 0     | Reference for transitions FROM Regime 1 |
| `alpha[kk,2,2]` | Fixed at 0     | Reference for transitions FROM Regime 2 |
| `gamma[1,pp]`   | Fixed at 0     | Regime 1 is the reference level         |
| `gamma0[2]`     | Truncated \> 0 | Regime 2 is the "high" regime           |

### Complete JAGS Model Code

Now we combine all the components into a single model string:

```{r jags-model-spec}
# Define JAGS model as a string
jags_model_string <- "
model{
for (pp in 1:P) {

## odds: transition probability matrices 
## midx: regime indicators

# 1st observation
odds[pp,1,1] <- 0.9
odds[pp,1,2] <- 0.1

midx[pp,1] ~ dcat(odds[pp,1,])

Y[pp,1] ~ dnorm(0, 1)
beta0[pp] ~ dnorm(beta00, tau_beta0)
beta1[pp] ~ dnorm(beta10, tau_beta1)
AR[pp] ~ dnorm(AR0, tau_AR)T(-1,1)
sd_noise[pp] ~ dnorm(sd_noise0, tau_sd_noise)
tau_noise[pp] = pow(sd_noise[pp], -2)
gamma[1,pp] = 0
gamma[2,pp] ~ dnorm(gamma0[2], tau_gamma[2])


# from T2 to maxT[pp]
 for (bb in 1:nrPeriod){
    
   intercept[pp,bb] = beta0[pp] + beta1[pp]*time[bb]
    
   for(tt in (nrObs[bb]+1):nrObs[bb+1]){

# from regime midx[pp,tt-1] to regime 1
odds[pp,tt,1] <- exp(alpha[1,1,midx[pp,tt-1]]+alpha[2,1,midx[pp,tt-1]]*Period2[tt]*G3[pp])
# from regime midx[pp,tt-1] to regime 2
odds[pp,tt,2] <- exp(alpha[1,2,midx[pp,tt-1]]+alpha[2,2,midx[pp,tt-1]]*Period2[tt]*G3[pp])

midx[pp,tt] ~ dcat(odds[pp,tt,])

mus[pp,tt] <- gamma[midx[pp,tt],pp] + intercept[pp,bb] + AR[pp] * (Y[pp,tt-1] - intercept[pp,bb] - gamma[midx[pp,tt-1],pp]) 

Y[pp,tt] ~ dnorm(mus[pp,tt], tau_noise[pp])


} # close loop over time points

}

} # close loop over persons

# priors
gamma0[2] ~ dnorm(0, 0.01)T(0,)
tau_gamma[2] ~ dgamma(0.001,0.001)
sd_gamma[2] = pow(tau_gamma[2], -1/2)

AR0 ~ dnorm(0,1)T(-1,1)
sd_AR ~ dunif(0, 1)
tau_AR = pow(sd_AR, -2)

sd_noise0 ~ dunif(0, 1)
sd_sd_noise ~ dunif(0, 1)
tau_sd_noise = pow(sd_sd_noise, -2)

beta00 ~ dnorm(0, 0.01)
tau_beta0 ~ dgamma(0.001,0.001)
sd_beta0 = pow(tau_beta0, -1/2)
beta10 ~ dnorm(0, 0.01)
tau_beta1 ~ dgamma(0.001,0.001)
sd_beta1 = pow(tau_beta1, -1/2)

for(kk in 1:2){
  alpha[kk,2,1] ~ dnorm(0,0.1)
  alpha[kk,1,1] <- 0
  alpha[kk,1,2] ~ dnorm(0,0.1)
  alpha[kk,2,2] <- 0
}


} # end of model
"

# Write model to a temporary file
model_file <- tempfile(fileext = ".txt")
writeLines(jags_model_string, model_file)
cat("JAGS model saved to:", model_file, "\n")
```

## Prepare Data for JAGS

```{r prepare-jags-data}
# Prepare data list for JAGS
jags_data <- list(
  Y = simdata$Y, 
  P = simdata$N,
  maxT = simdata$O,
  Period2 = simdata$Period2,
  time = simdata$time, 
  nrObs = simdata$nrObs,
  nrPeriod = simdata$nrPeriod,
  G3 = simdata$G3
)

# Display data structure
str(jags_data)
```

## MCMC Settings

```{r mcmc-settings}
# Fixed initial values for reproducibility
fixedinits <- list(
  list(.RNG.seed = 1, .RNG.name = "base::Mersenne-Twister"),
  list(.RNG.seed = 2, .RNG.name = "base::Mersenne-Twister")
)

# MCMC settings
n_chains <- 2        # Number of MCMC chains
n_adapt <- 4000      # Adaptation iterations
n_burnin <- 1000     # Burn-in iterations
n_iter <- 20000      # Sampling iterations

# Parameters to monitor
parameterlist <- c(
  "beta00", "sd_beta0",         # GCM intercept parameters
  "beta10", "sd_beta1",         # GCM slope parameters
  "AR0", "sd_AR",               # AR parameters
  "sd_noise0", "sd_sd_noise",   # Process noise parameters
  "gamma0", "sd_gamma",         # Regime shift parameters
  "AR", "sd_noise",             # Person-specific parameters
  "alpha",                      # Transition parameters
  "midx",                       # Regime indicators
  "deviance"                    # Model fit
)
```

## Run JAGS Model

```{r run-jags, results='hide'}
# Initialize JAGS model
cat("Initializing JAGS model...\n")
jagsModel <- jags.model(
  file = model_file, 
  data = jags_data, 
  inits = fixedinits, 
  n.chains = n_chains, 
  n.adapt = n_adapt
)

# Burn-in period
cat("Running burn-in (", n_burnin, " iterations)...\n", sep = "")
update(jagsModel, n.iter = n_burnin)

# Sample from posterior
cat("Sampling from posterior (", n_iter, " iterations)...\n", sep = "")
codaSamples <- coda.samples(
  jagsModel, 
  variable.names = parameterlist,
  n.iter = n_iter
)

cat("MCMC sampling complete.\n")
```

# Part 3: Post-Analysis

## Generate Summary Table

```{r summary-table}
# Generate comprehensive summary using zcalc
resulttable <- zcalc(codaSamples)

# Display fixed-effect parameters
fixed_params <- c("beta00", "sd_beta0", "beta10", "sd_beta1",
                  "AR0", "sd_AR", "sd_noise0", "sd_sd_noise",
                  "gamma0[2]", "sd_gamma[2]",
                  "alpha[1,2,1]", "alpha[2,2,1]", 
                  "alpha[1,1,2]", "alpha[2,1,2]")

# Extract rows for fixed parameters
fixed_rows <- resulttable[rownames(resulttable) %in% fixed_params, ]

cat("\n=== Fixed-Effect Parameter Estimates ===\n")
print(fixed_rows[, c("mean", "sd", "95% HDI_L", "95% HDI_H", "ESS", "RHAT")])
```

## Parameter Recovery Assessment

```{r parameter-recovery}
# True parameter values
# Note: gamma0[2] corresponds to Δ_{μ,2} in the paper (regime shift magnitude)
true_values <- c(
  beta00 = simdata$beta00,
  sd_beta0 = simdata$sd_beta0,
  beta10 = simdata$beta10,
  sd_beta1 = simdata$sd_beta1,
  AR0 = simdata$AR0,
  sd_AR = simdata$sd_AR,
  sd_noise0 = simdata$sd_noise0,
  sd_sd_noise = simdata$sd_sd_noise,
  `gamma0[2]` = simdata$gamma0[2],
  sd_gamma_2 = simdata$sd_gamma[2],
  `alpha[1,2,1]` = simdata$alpha[1, 2, 1],
  `alpha[2,2,1]` = simdata$alpha[2, 2, 1],
  `alpha[1,1,2]` = simdata$alpha[1, 1, 2],
  `alpha[2,1,2]` = simdata$alpha[2, 1, 2]
)

# Create comparison table
param_names <- names(true_values)
recovery_table <- data.frame(
  Parameter = param_names,
  True = unname(true_values),
  Estimate = resulttable[match(param_names, rownames(resulttable)), "mean"],
  SD = resulttable[match(param_names, rownames(resulttable)), "sd"],
  HDI_Low = resulttable[match(param_names, rownames(resulttable)), "95% HDI_L"],
  HDI_High = resulttable[match(param_names, rownames(resulttable)), "95% HDI_H"],
  stringsAsFactors = FALSE
)

# Calculate bias and coverage
recovery_table$Bias <- recovery_table$Estimate - recovery_table$True
recovery_table$Covered <- with(recovery_table, 
                               True >= HDI_Low & True <= HDI_High)

cat("\n=== Parameter Recovery Summary ===\n")
print(recovery_table, row.names = FALSE)
```

## Model Fit and Entropy

This section computes model fit diagnostics including Information Criteria (AIC, BIC, sBIC) and Entropy.

### Information Criteria

Information Criteria (IC) measures are used for model comparison, balancing goodness-of-fit against model complexity. We compute three commonly used IC measures:

$$
\text{AIC} = -2\ln(\hat{L}) + 2k
$$

$$
\text{BIC} = -2\ln(\hat{L}) + k\ln(n)
$$

$$
\text{sBIC} = -2\ln(\hat{L}) + k\ln\left(\frac{n(n+2)}{24}\right)
$$

where:

-   $\hat{L}$ is the maximum likelihood (or in Bayesian context, we use the deviance $= -2\ln(\hat{L})$)
-   $k$ is the number of model parameters
-   $n = N \times T$ is the total number of observations

**Interpretation:**

-   **Lower IC values** indicate better model fit
-   An IC difference **\> 2** between models is considered meaningful
-   An IC difference **\> 10** provides strong evidence for the better-fitting model

The three IC measures impose different penalties for complexity:

| Measure | Complexity Penalty   | Tendency                      |
|---------|----------------------|-------------------------------|
| AIC     | Lightest ($2k$)      | May favor more complex models |
| BIC     | Heaviest ($k\ln(n)$) | May favor simpler models      |
| sBIC    | Moderate             | Compromise for finite samples |

### Entropy

Entropy is a measure of **regime classification clarity** that quantifies how well the model separates observations into distinct regimes. We adopt the entropy definition used in mixture modeling (Asparouhov & Muthén, 2014; Celeux & Soromenho, 1996):

$$
E_K = 1 + \frac{1}{n \ln(K)} \sum_{i=1}^{N} \sum_{t=1}^{T_i} \sum_{k=1}^{K} \hat{p}_{itk} \ln(\hat{p}_{itk})
$$

where:

-   $N$ is the number of individuals
-   $T_i$ is the number of time points for person $i$
-   $K$ is the number of regimes (K=2 in our case)
-   $\hat{p}_{itk}$ is the **posterior probability** of person $i$ being in regime $k$ at time $t$
-   $n = \sum_{i=1}^{N} T_i$ is the total number of observations

For a 2-regime model (K=2), this simplifies to:

$$
E_2 = 1 + \frac{\sum_{i=1}^{N} \sum_{t=1}^{T} \left[ \hat{p}_{it,2} \ln(\hat{p}_{it,2}) + \hat{p}_{it,1} \ln(\hat{p}_{it,1}) \right]}{N \cdot T \cdot \ln(2)}
$$

**Why does this formula work?**

The term $\sum p \ln(p)$ is the **negative Shannon entropy**. 

- When all probabilities are either 0 or 1 (perfect classification), $p \ln(p) = 0$ (since $\lim_{p \to 0} p \ln(p) = 0$), giving $E_K = 1$. 
- When K=2 and all probabilities are 0.5 (maximum uncertainty), each observation contributes $0.5 \ln(0.5) + 0.5 \ln(0.5) = \ln(0.5)$, so: $$E_2 = 1 + \frac{n \cdot \ln(0.5)}{n \cdot \ln(2)} = 1 + \frac{\ln(0.5)}{\ln(2)} = 1 - 1 = 0$$

**Interpretation of entropy values:**

| Entropy Range | Classification Quality                             |
|---------------|----------------------------------------------------|
| 0.9 - 1.0     | Excellent separation                               |
| 0.8 - 0.9     | Good separation                                    |
| 0.6 - 0.8     | Acceptable for RS models with frequent transitions |
| \< 0.6        | Poor separation (regimes not well distinguished)   |

**Note:** Conventional thresholds (e.g., > 0.9) developed for cross-sectional mixture models (Celeux & Soromenho, 1996) may be overly stringent for longitudinal RS models with frequent transitions. Our simulation results (see Section 5.4 of Li et al., 2026) suggest that entropy values as low as 0.6 can still yield satisfactory parameter estimation and classification performance. The recommended model selection strategy combines entropy (threshold > 0.6) with IC measures.

```{r model-fit-entropy}
# ============================================
# STEP 1: Extract posterior regime probabilities
# ============================================
# The JAGS output contains midx[i,t] which represents the regime indicator
# The posterior mean of midx gives us the expected regime (1 or 2)
# We convert this to probability of being in regime 2: pr2 = E[midx] - 1

midx_rows <- grep("^midx", rownames(resulttable))
pr2 <- resulttable[midx_rows, "mean"] - 1  # Posterior probability of regime 2
pr1 <- 1 - pr2                              # Probability of regime 1 (complement)

# ============================================
# STEP 2: Calculate Entropy
# ============================================
# Formula: E_K = 1 + sum(p*log(p)) / (N*T*log(K))
# For K=2: E_2 = 1 + [sum(pr1*log(pr1)) + sum(pr2*log(pr2))] / (N*T*log(2))
#
# Note: 0*log(0) is defined as 0 (using na.rm=TRUE handles this)

N <- simdata$N  # Number of persons
O <- simdata$O  # Number of time points

# Calculate entropy using the formula from the paper
entropy <- 1 + (sum(pr2 * log(pr2), na.rm = TRUE) + 
                sum(pr1 * log(pr1), na.rm = TRUE)) / (N * O * log(2))

# ============================================
# STEP 3: Calculate Information Criteria
# ============================================
# Extract deviance from JAGS output (deviance = -2 * log-likelihood)
deviance <- resulttable[grep("deviance", rownames(resulttable)), "mean"]

# Count the number of estimated parameters (npar) for GCM-RS2 model:
# 
# Population-level fixed effects:
#   - GCM: beta00, beta10 = 2
#   - AR: AR0 = 1
#   - Process noise: sd_noise0 = 1
#   - Regime shift: gamma0[2] = 1 (gamma0[1] is fixed at 0)
#   Subtotal: 5
#
# Random effect standard deviations:
#   - GCM: sd_beta0, sd_beta1 = 2
#   - AR: sd_AR = 1
#   - Process noise: sd_sd_noise = 1
#   - Regime shift: sd_gamma[2] = 1 (sd_gamma[1] is 0)
#   Subtotal: 5
#
# RS transition log-odds coefficients:
#   - alpha[1,2,1]: baseline log-odds of 1->2 = 1
#   - alpha[2,2,1]: intervention effect on 1->2 = 1
#   - alpha[1,1,2]: baseline log-odds of 2->1 = 1
#   - alpha[2,1,2]: intervention effect on 2->1 = 1
#   (alpha[k,1,1] and alpha[k,2,2] are fixed at 0)
#   Subtotal: 4
#
# Total: 5 + 5 + 4 = 14 parameters
npar <- 14

# Calculate IC measures using formulas from the paper (Equations 8-10)
# Note: In Bayesian context, we use posterior mean deviance
n_total <- N * O  # Total number of observations

AIC <- 2 * npar + deviance                           # Equation 8
BIC <- npar * log(n_total) + deviance                # Equation 9
sBIC <- npar * log(n_total * (n_total + 2) / 24) + deviance  # Equation 10

# ============================================
# STEP 4: Display Results
# ============================================
cat("\n=== Model Fit Diagnostics ===\n")
cat("\n--- Information Criteria ---\n")
cat("AIC:", round(AIC, 2), "\n")
cat("BIC:", round(BIC, 2), "\n")
cat("sBIC (sample-size adjusted BIC):", round(sBIC, 2), "\n")
cat("  Note: Lower values indicate better fit; differences > 2 are meaningful\n")

cat("\n--- Entropy ---\n")
cat("Entropy:", round(entropy, 4), "\n")
cat("  (Values closer to 1 indicate clearer regime separation;\n")
cat("   values > 0.6 are generally considered acceptable for RS models)\n")

cat("\nDeviance:", round(deviance, 2), "\n")

# Create summary table of model fit diagnostics
fit_summary <- data.frame(
  Metric = c("AIC", "BIC", "sBIC", "Entropy", "Deviance"),
  Value = c(round(AIC, 2), round(BIC, 2), round(sBIC, 2), 
            round(entropy, 4), round(deviance, 2)),
  Description = c(
    "Akaike Information Criterion (Eq. 8)",
    "Bayesian Information Criterion (Eq. 9)", 
    "Sample-size adjusted BIC (Eq. 10)",
    "Regime classification clarity (0-1)",
    "-2 * log-likelihood"
  )
)

cat("\n--- Summary Table ---\n")
print(fit_summary, row.names = FALSE)
```

## Classification Accuracy

```{r classification-accuracy}
# Compare estimated vs. true regime indicators
midx_est <- resulttable[midx_rows, "median"]
midx_true <- as.vector(simdata$midx)

# Confusion matrix
TP <- sum(midx_true == 2 & round(midx_est) == 2)
TN <- sum(midx_true == 1 & round(midx_est) == 1)
FP <- sum(midx_true == 1 & round(midx_est) == 2)
FN <- sum(midx_true == 2 & round(midx_est) == 1)

# Calculate metrics
accuracy <- (TP + TN) / (N * O)
recall <- TP / (TP + FN)
precision <- TP / (TP + FP)

cat("\n=== Regime Classification Performance ===\n")
cat("Accuracy:", round(accuracy, 4), "\n")
cat("Recall (Sensitivity):", round(recall, 4), "\n")
cat("Precision:", round(precision, 4), "\n")
```

## Convergence Diagnostics

```{r convergence-plot, fig.cap="Trace plots and density plots for key parameters."}
# Select key parameters for visualization
key_params <- c("beta00", "beta10", "AR0", "gamma0[2]", "alpha[1,2,1]", "alpha[2,2,1]")

# Extract samples for key parameters
samples_subset <- codaSamples[, key_params]

# Plot trace and density
plot(samples_subset)
```

## RHAT Summary

```{r rhat-summary}
# Check convergence via RHAT
max_rhat <- max(resulttable$RHAT, na.rm = TRUE)
n_high_rhat <- sum(resulttable$RHAT > 1.1, na.rm = TRUE)

cat("\n=== Convergence Summary ===\n")
cat("Maximum RHAT:", round(max_rhat, 4), "\n")
cat("Parameters with RHAT > 1.1:", n_high_rhat, "\n")

if (max_rhat < 1.1) {
  cat("Conclusion: All chains appear to have converged.\n")
} else {
  cat("Warning: Some parameters may not have converged. Consider longer runs.\n")
}
```

# Summary

This tutorial demonstrated:

1.  **Data Generation**: How to simulate data from the GCM-RS2 model with gradual growth trends and regime-switching dynamics.

2.  **Model Specification**: The JAGS model syntax implementing the hierarchical structure with person-specific parameters and regime transitions.

3.  **Model Fitting**: Using `rjags` to fit the Bayesian model with appropriate MCMC settings.

    -   Adaptation: `r n_adapt` iterations
    -   Burn-in: `r n_burnin` iterations\
    -   Sampling: `r n_iter` iterations per chain
    -   Chains: `r n_chains`

4.  **Post-Analysis**: Comprehensive summary statistics including HDIs, effective sample sizes, RHAT convergence diagnostics, entropy measures, and classification accuracy metrics.

The results demonstrate satisfactory parameter recovery and regime classification under the simulated conditions with $N = `r simdata$N`$ and $T =`r simdata$O`$.

# Session Information

```{r session-info}
sessionInfo()
```

# References

-   Asparouhov, T., & Muthén, B. (2014). Variable-specific entropy contribution. Retrieved from http://www.statmodel.com/download/UnivariateEntropy.pdf
-   Celeux, G., & Soromenho, G. (1996). An entropy criterion for assessing the number of clusters in a mixture model. *Journal of Classification*, 13, 195–212.
-   Kruschke, J. (2014). *Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan*. Academic Press.
-   Li, Y., Xiong, X., Oravecz, Z., & Chow, S.-M. (2026). Simultaneous detection of gradual and abrupt structural changes in Bayesian longitudinal modelling. *British Journal of Mathematical and Statistical Psychology*.
-   Li, Y., Williams, L., Muth, C., Heshmati, S., Chow, S.-M., & Oravecz, Z. (2024). A growth of hierarchical autoregression model for capturing individual differences in changes of dynamic characteristics of psychological processes. *Structural Equation Modeling: A Multidisciplinary Journal*, 32, 1–14.
-   Plummer, M. (2003). JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling. In *Proceedings of the 3rd International Workshop on Distributed Statistical Computing*.
